@book{cohen1995empirical,
author = {Cohen, Paul R},
file = {:home/lasinac/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen - 1995 - Empirical methods for artificial intelligence.pdf:pdf},
publisher = {MIT press Cambridge, MA},
title = {{Empirical methods for artificial intelligence}},
volume = {139},
year = {1995}
}
@techreport{Huang,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convo-lutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1) 2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q},
file = {:home/lasinac/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - Unknown - Densely Connected Convolutional Networks.pdf:pdf},
title = {{Densely Connected Convolutional Networks}},
url = {https://github.com/liuzhuang13/DenseNet.}
}
@inproceedings{Huang2016,
abstract = {Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the i dentity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91{\%} on CIFAR-10).},
archivePrefix = {arXiv},
arxivId = {1603.09382},
author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46493-0_39},
eprint = {1603.09382},
file = {:home/lasinac/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2016 - Deep networks with stochastic depth.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
pages = {646--661},
publisher = {Springer Verlag},
title = {{Deep networks with stochastic depth}},
volume = {9908 LNCS},
year = {2016}
}
